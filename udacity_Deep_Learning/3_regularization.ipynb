{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic Regression with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "betaL2 = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    bias1 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Training\n",
    "    logits = tf.matmul(tf_train_dataset, weights1) + bias1\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) \\\n",
    "        + betaL2*tf.nn.l2_loss(weights1))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_dataset, weights1) + bias1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_dataset, weights1) + bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.564045\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 16.9%\n",
      "Minibatch loss at step 500: 3.231931\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 1.815189\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 1500: 1.275937\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2000: 0.795109\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2500: 0.702781\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.659449\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 3500: 0.875648\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 4000: 0.655327\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 4500: 0.742849\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NN with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with nodes = 256 and L2 of 0.0001000000\n",
      "Test accuracy: 89.7%\n",
      "Initialized with nodes = 256 and L2 of 0.0005000000\n",
      "Test accuracy: 92.5%\n",
      "Initialized with nodes = 256 and L2 of 0.0010000000\n",
      "Test accuracy: 93.5%\n",
      "Initialized with nodes = 256 and L2 of 0.0050000000\n",
      "Test accuracy: 91.9%\n",
      "Initialized with nodes = 256 and L2 of 0.0100000000\n",
      "Test accuracy: 90.4%\n",
      "Initialized with nodes = 512 and L2 of 0.0001000000\n",
      "Test accuracy: 90.4%\n",
      "Initialized with nodes = 512 and L2 of 0.0005000000\n",
      "Test accuracy: 92.8%\n",
      "Initialized with nodes = 512 and L2 of 0.0010000000\n",
      "Test accuracy: 93.8%\n",
      "Initialized with nodes = 512 and L2 of 0.0050000000\n",
      "Test accuracy: 92.0%\n",
      "Initialized with nodes = 512 and L2 of 0.0100000000\n",
      "Test accuracy: 90.3%\n",
      "Initialized with nodes = 1024 and L2 of 0.0001000000\n",
      "Test accuracy: 90.5%\n",
      "Initialized with nodes = 1024 and L2 of 0.0005000000\n",
      "Test accuracy: 93.3%\n",
      "Initialized with nodes = 1024 and L2 of 0.0010000000\n",
      "Test accuracy: 94.0%\n",
      "Initialized with nodes = 1024 and L2 of 0.0050000000\n",
      "Test accuracy: 92.0%\n",
      "Initialized with nodes = 1024 and L2 of 0.0100000000\n",
      "Test accuracy: 90.4%\n",
      "Initialized with nodes = 2048 and L2 of 0.0001000000\n",
      "Test accuracy: 91.0%\n",
      "Initialized with nodes = 2048 and L2 of 0.0005000000\n",
      "Test accuracy: 93.6%\n",
      "Initialized with nodes = 2048 and L2 of 0.0010000000\n",
      "Test accuracy: 94.1%\n",
      "Initialized with nodes = 2048 and L2 of 0.0050000000\n",
      "Test accuracy: 91.8%\n",
      "Initialized with nodes = 2048 and L2 of 0.0100000000\n",
      "Test accuracy: 90.5%\n",
      "Initialized with nodes = 4096 and L2 of 0.0001000000\n",
      "Test accuracy: 92.0%\n",
      "Initialized with nodes = 4096 and L2 of 0.0005000000\n",
      "Test accuracy: 94.0%\n",
      "Initialized with nodes = 4096 and L2 of 0.0010000000\n",
      "Test accuracy: 94.4%\n",
      "Initialized with nodes = 4096 and L2 of 0.0050000000\n",
      "Test accuracy: 92.0%\n",
      "Initialized with nodes = 4096 and L2 of 0.0100000000\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes_cases = [256, 512, 1024, 2048, 4096]\n",
    "betaL2s = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "perf = np.empty((len(num_hidden_nodes_cases), len(betaL2s)))\n",
    "\n",
    "for i in range(len(num_hidden_nodes_cases)):\n",
    "    num_hidden_nodes = num_hidden_nodes_cases[i]\n",
    "    for j in range(len(betaL2s)):\n",
    "        betaL2 = betaL2s[j]\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "\n",
    "            # Input data. For the training data, we use a placeholder that will be fed\n",
    "            # at run time with a training minibatch.\n",
    "            tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                            shape=(batch_size, image_size * image_size))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            tf_valid_dataset = tf.constant(valid_dataset)\n",
    "            tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "            # Variables.\n",
    "            weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_hidden_nodes]))\n",
    "            bias1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "            weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "            bias2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "            #Training\n",
    "            lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + bias1)\n",
    "            logits = tf.matmul(lay1_train, weights2) + bias2\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) \\\n",
    "                + betaL2*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "\n",
    "            # Optimizer.\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            train_prediction = tf.nn.softmax(logits)\n",
    "            lay1_val = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + bias1)\n",
    "            valid_prediction = tf.nn.softmax(tf.matmul(lay1_val, weights2) + bias2)\n",
    "            lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + bias1)\n",
    "            test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + bias2)\n",
    "\n",
    "        num_steps = 5000\n",
    "\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print(\"Initialized with nodes = %d and L2 of %.10f\" % (num_hidden_nodes, betaL2))\n",
    "            for step in range(num_steps):\n",
    "                # Pick an offset within the training data, which has been randomized.\n",
    "                # Note: we could use better randomization across epochs.\n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                # Generate a minibatch.\n",
    "                batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "                # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "                # and the value is the numpy array to feed to it.\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "                _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                if (step % 500 == 0):\n",
    "                    #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                    #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                    #print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                    continue\n",
    "\n",
    "            acc = accuracy(test_prediction.eval(), test_labels)  \n",
    "            print(\"Test accuracy: %.1f%%\" % acc)\n",
    "\n",
    "        perf[i,j] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fcc4911a990>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAGJCAYAAAD8L4t3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc49576250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes()\n",
    "sns.heatmap(perf, xticklabels=betaL2s, yticklabels=num_hidden_nodes_cases, annot=True, fmt='.1f')\n",
    "ax.set_xlabel('Regularization Weight')\n",
    "ax.set_ylabel('Number of Hidden Nodes')\n",
    "ax.set_title('Model Accuracy on Test Data')\n",
    "#plt.set_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with nodes = 1024 and L2 of 0.0000000000\n",
      "Minibatch loss at step 0: 398.138214\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 33.8%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 3500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 4000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 4500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.7%\n",
      "Test accuracy: 81.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "betaL2 = 0.0#1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_hidden_nodes]))\n",
    "    bias1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    bias2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    #Training\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + bias1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + bias2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) \\\n",
    "        + betaL2*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_val = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + bias1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_val, weights2) + bias2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + bias1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + bias2)\n",
    "\n",
    "num_steps = 5000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized with nodes = %d and L2 of %.10f\" % (num_hidden_nodes, betaL2))\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        batch_i = np.random.choice([1,20,300,4000])\n",
    "        offset = (batch_i * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            continue\n",
    "\n",
    "    acc = accuracy(test_prediction.eval(), test_labels)  \n",
    "    print(\"Test accuracy: %.1f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "betaL2 = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    #keepProb = tf.placeholder(tf.float32)\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_hidden_nodes]))\n",
    "    bias1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    bias2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    #Training\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + bias1)\n",
    "    lay1_dropout = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(lay1_dropout, weights2) + bias2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) \\\n",
    "        + betaL2*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_val = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + bias1)\n",
    "    lay1_val = tf.nn.dropout(lay1_val, 1.0)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_val, weights2) + bias2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + bias1)\n",
    "    lay1_test = tf.nn.dropout(lay1_test, 1.0)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with nodes = 1024 and L2 of 0.0010000000\n",
      "Minibatch loss at step 0: 854.843567\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 33.8%\n",
      "Minibatch loss at step 500: 219.219696\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1000: 116.647293\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1500: 69.712112\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2000: 41.398045\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2500: 25.070587\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 3000: 15.431509\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3500: 9.703618\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 4000: 6.009991\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 4500: 3.972458\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 5000: 2.554586\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 5500: 1.828799\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 6000: 1.362692\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6500: 1.271394\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 7000: 0.807651\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7500: 0.578687\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 8000: 0.828134\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 8500: 0.844478\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 9000: 0.624591\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 9500: 0.554085\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Test accuracy: 93.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized with nodes = %d and L2 of %.10f\" % (num_hidden_nodes, betaL2))\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #batch_i = np.random.choice([1,20,300,4000])\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            continue\n",
    "\n",
    "    acc = accuracy(test_prediction.eval(), test_labels)  \n",
    "    print(\"Test accuracy: %.1f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the base case, it seems like we sacrifice a little accuracy after trianing for the same number of steps. In the extreme overfitting case, we get a significant boost in accuracy though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "l1_nodes = 1024\n",
    "l2_nodes = 1024\n",
    "l3_nodes = 1024\n",
    "l4_nodes = 1024\n",
    "betaL2 = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    #keepProb = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0) #Count the steps taken thus far\n",
    "    l1_weights = tf.Variable(tf.truncated_normal([image_size*image_size, l1_nodes], stddev=np.sqrt(2.0/l1_nodes)))\n",
    "    l1_bias = tf.Variable(tf.zeros([l1_nodes]))\n",
    "    \n",
    "    l2_weights = tf.Variable(tf.truncated_normal([l1_nodes, l2_nodes], stddev=np.sqrt(2.0/l2_nodes)))\n",
    "    l2_bias = tf.Variable(tf.zeros([l2_nodes]))\n",
    "    \n",
    "    l3_weights = tf.Variable(tf.truncated_normal([l2_nodes, l3_nodes], stddev=np.sqrt(2.0/l3_nodes)))\n",
    "    l3_bias = tf.Variable(tf.zeros([l3_nodes]))\n",
    "    \n",
    "    l4_weights = tf.Variable(tf.truncated_normal([l3_nodes, l4_nodes], stddev=np.sqrt(2.0/l4_nodes)))\n",
    "    l4_bias = tf.Variable(tf.zeros([l4_nodes]))\n",
    "    \n",
    "    out_weights = tf.Variable(tf.truncated_normal([l4_nodes, num_labels], stddev=np.sqrt(2.0/l4_nodes)))\n",
    "    out_bias = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    #Training\n",
    "    l1_train = tf.nn.relu(tf.matmul(tf_train_dataset, l1_weights) + l1_bias)\n",
    "    l1_dropout = tf.nn.dropout(l1_train, 0.5)\n",
    "    l2_train = tf.nn.relu(tf.matmul(l1_dropout, l2_weights) + l2_bias)\n",
    "    l2_dropout = tf.nn.dropout(l2_train, 0.5)\n",
    "    l3_train = tf.nn.relu(tf.matmul(l2_dropout, l3_weights) + l3_bias)\n",
    "    l3_dropout = tf.nn.dropout(l3_train, 0.5)\n",
    "    l4_train = tf.nn.relu(tf.matmul(l3_dropout, l4_weights) + l4_bias)\n",
    "    l4_dropout = tf.nn.dropout(l4_train, 0.5)\n",
    "    \n",
    "    logits = tf.matmul(l4_dropout, out_weights) + out_bias\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) \\\n",
    "        + betaL2*(tf.nn.l2_loss(l1_weights) + \n",
    "                  tf.nn.l2_loss(l2_weights) + \n",
    "                  tf.nn.l2_loss(l3_weights) + \n",
    "                  tf.nn.l2_loss(l4_weights) + \n",
    "                  tf.nn.l2_loss(out_weights)))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.9)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    l1_val = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_valid_dataset, l1_weights) + l1_bias), 1.0)\n",
    "    l2_val = tf.nn.dropout(tf.nn.relu(tf.matmul(l1_val, l2_weights) + l2_bias), 1.0)\n",
    "    l3_val = tf.nn.dropout(tf.nn.relu(tf.matmul(l2_val, l3_weights) + l3_bias), 1.0)\n",
    "    l4_val = tf.nn.dropout(tf.nn.relu(tf.matmul(l3_val, l4_weights) + l4_bias), 1.0)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(l4_val, out_weights) + out_bias)\n",
    "    l1_test = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_test_dataset, l1_weights) + l1_bias), 1.0)\n",
    "    l2_test = tf.nn.dropout(tf.nn.relu(tf.matmul(l1_test, l2_weights) + l2_bias), 1.0)\n",
    "    l3_test = tf.nn.dropout(tf.nn.relu(tf.matmul(l2_test, l3_weights) + l3_bias), 1.0)\n",
    "    l4_test = tf.nn.dropout(tf.nn.relu(tf.matmul(l3_test, l4_weights) + l4_bias), 1.0)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(l4_test, out_weights) + out_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with nodes = 1024 and L2 of 0.0010000000\n",
      "Current learning rate = 0.50000\n",
      "Minibatch loss at step 0: 6.056422\n",
      "Minibatch accuracy: 8.2%\n",
      "Validation accuracy: 17.7%\n",
      "Current learning rate = 0.40500\n",
      "Minibatch loss at step 2000: 0.885246\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 88.7%\n",
      "Current learning rate = 0.32805\n",
      "Minibatch loss at step 4000: 0.613010\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 89.8%\n",
      "Current learning rate = 0.26572\n",
      "Minibatch loss at step 6000: 0.511973\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Current learning rate = 0.21523\n",
      "Minibatch loss at step 8000: 0.444938\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 90.7%\n",
      "Current learning rate = 0.17434\n",
      "Minibatch loss at step 10000: 0.420176\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 91.1%\n",
      "Current learning rate = 0.14121\n",
      "Minibatch loss at step 12000: 0.485543\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 91.2%\n",
      "Current learning rate = 0.11438\n",
      "Minibatch loss at step 14000: 0.369352\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 91.2%\n",
      "Current learning rate = 0.09265\n",
      "Minibatch loss at step 16000: 0.415837\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 91.3%\n",
      "Current learning rate = 0.07505\n",
      "Minibatch loss at step 18000: 0.414332\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Current learning rate = 0.06079\n",
      "Minibatch loss at step 20000: 0.349720\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 91.7%\n",
      "Current learning rate = 0.04924\n",
      "Minibatch loss at step 22000: 0.432246\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.5%\n",
      "Current learning rate = 0.03988\n",
      "Minibatch loss at step 24000: 0.368269\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Current learning rate = 0.03231\n",
      "Minibatch loss at step 26000: 0.336395\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 91.9%\n",
      "Current learning rate = 0.02617\n",
      "Minibatch loss at step 28000: 0.357939\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 91.8%\n",
      "Current learning rate = 0.02120\n",
      "Minibatch loss at step 30000: 0.310369\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 91.8%\n",
      "Current learning rate = 0.01717\n",
      "Minibatch loss at step 32000: 0.267268\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 91.8%\n",
      "Current learning rate = 0.01391\n",
      "Minibatch loss at step 34000: 0.348239\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.9%\n",
      "Current learning rate = 0.01126\n",
      "Minibatch loss at step 36000: 0.286924\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.1%\n",
      "Current learning rate = 0.00912\n",
      "Minibatch loss at step 38000: 0.369385\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 92.1%\n",
      "Current learning rate = 0.00739\n",
      "Minibatch loss at step 40000: 0.321541\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00599\n",
      "Minibatch loss at step 42000: 0.297616\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00485\n",
      "Minibatch loss at step 44000: 0.309725\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00393\n",
      "Minibatch loss at step 46000: 0.320597\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00318\n",
      "Minibatch loss at step 48000: 0.292308\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00258\n",
      "Minibatch loss at step 50000: 0.316757\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00209\n",
      "Minibatch loss at step 52000: 0.320927\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00169\n",
      "Minibatch loss at step 54000: 0.326775\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.1%\n",
      "Current learning rate = 0.00137\n",
      "Minibatch loss at step 56000: 0.322932\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00111\n",
      "Minibatch loss at step 58000: 0.266526\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00090\n",
      "Minibatch loss at step 60000: 0.269282\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00073\n",
      "Minibatch loss at step 62000: 0.312187\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00059\n",
      "Minibatch loss at step 64000: 0.308867\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00048\n",
      "Minibatch loss at step 66000: 0.295391\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00039\n",
      "Minibatch loss at step 68000: 0.279670\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00031\n",
      "Minibatch loss at step 70000: 0.330206\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00025\n",
      "Minibatch loss at step 72000: 0.271562\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00021\n",
      "Minibatch loss at step 74000: 0.289377\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00017\n",
      "Minibatch loss at step 76000: 0.255966\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00013\n",
      "Minibatch loss at step 78000: 0.331842\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00011\n",
      "Minibatch loss at step 80000: 0.281228\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00009\n",
      "Minibatch loss at step 82000: 0.286509\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00007\n",
      "Minibatch loss at step 84000: 0.259597\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00006\n",
      "Minibatch loss at step 86000: 0.273714\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00005\n",
      "Minibatch loss at step 88000: 0.353881\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00004\n",
      "Minibatch loss at step 90000: 0.287055\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00003\n",
      "Minibatch loss at step 92000: 0.312798\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00002\n",
      "Minibatch loss at step 94000: 0.303316\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00002\n",
      "Minibatch loss at step 96000: 0.260430\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00002\n",
      "Minibatch loss at step 98000: 0.295990\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00001\n",
      "Minibatch loss at step 100000: 0.291359\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00001\n",
      "Minibatch loss at step 102000: 0.366787\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00001\n",
      "Minibatch loss at step 104000: 0.295233\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00001\n",
      "Minibatch loss at step 106000: 0.301383\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00001\n",
      "Minibatch loss at step 108000: 0.298061\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 110000: 0.295181\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 112000: 0.318434\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 114000: 0.284762\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 116000: 0.301789\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 118000: 0.286051\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 120000: 0.288629\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 122000: 0.297490\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 124000: 0.297808\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 126000: 0.366088\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 128000: 0.347137\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 130000: 0.277180\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 132000: 0.281451\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 134000: 0.254424\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 136000: 0.307255\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 138000: 0.322045\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 140000: 0.290156\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 142000: 0.244048\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 144000: 0.309241\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 146000: 0.326747\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 148000: 0.263357\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 150000: 0.310687\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 152000: 0.284307\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 154000: 0.297106\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 156000: 0.241327\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 158000: 0.285993\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 160000: 0.350082\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 162000: 0.289848\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 164000: 0.303616\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 166000: 0.264670\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 168000: 0.320765\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 170000: 0.341573\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 172000: 0.299883\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 174000: 0.283658\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 176000: 0.340890\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 178000: 0.325779\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 180000: 0.271181\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 182000: 0.286861\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 184000: 0.327989\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 186000: 0.273531\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 188000: 0.297718\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 190000: 0.293329\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 192000: 0.315612\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 194000: 0.324742\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 196000: 0.330396\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 198000: 0.290789\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 200000: 0.265673\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 202000: 0.284687\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 204000: 0.259894\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 206000: 0.289881\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 208000: 0.291905\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 210000: 0.267581\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 212000: 0.311580\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 214000: 0.296927\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 216000: 0.279842\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 218000: 0.338954\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 220000: 0.298583\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 222000: 0.291815\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 224000: 0.299342\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 226000: 0.266514\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 228000: 0.289291\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 230000: 0.293821\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 232000: 0.317081\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 234000: 0.315388\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 236000: 0.304178\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 238000: 0.301199\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 240000: 0.269841\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 242000: 0.272751\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 244000: 0.320144\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 246000: 0.332723\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 248000: 0.294914\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 250000: 0.309894\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 252000: 0.297017\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 254000: 0.267016\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 256000: 0.250293\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 258000: 0.311383\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 260000: 0.330786\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 262000: 0.347396\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 264000: 0.332237\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 266000: 0.346123\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 268000: 0.356479\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 270000: 0.323180\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 272000: 0.319830\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 274000: 0.304850\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 276000: 0.250786\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 278000: 0.312064\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 280000: 0.313111\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 282000: 0.272606\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 284000: 0.257180\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 286000: 0.246919\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 288000: 0.298162\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 290000: 0.311251\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 292000: 0.291822\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 294000: 0.282646\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 296000: 0.298816\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 298000: 0.277915\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 300000: 0.347807\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 302000: 0.290912\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 304000: 0.254416\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 306000: 0.277023\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 308000: 0.270183\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 310000: 0.297863\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 312000: 0.301276\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 314000: 0.277095\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 316000: 0.306645\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 318000: 0.264149\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 320000: 0.307760\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 322000: 0.330758\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 324000: 0.330363\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 326000: 0.298796\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 328000: 0.307758\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 330000: 0.296974\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 332000: 0.291748\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 334000: 0.314073\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 336000: 0.298365\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 338000: 0.297752\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 340000: 0.264312\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 342000: 0.307822\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 344000: 0.294758\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 346000: 0.285347\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 348000: 0.264138\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 350000: 0.305325\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 352000: 0.254935\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 354000: 0.284853\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 356000: 0.303574\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 358000: 0.276054\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 360000: 0.280861\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 362000: 0.283626\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 364000: 0.317679\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 366000: 0.335436\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 368000: 0.320245\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 370000: 0.280688\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 372000: 0.291673\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 374000: 0.308849\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 376000: 0.272604\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 378000: 0.269029\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 380000: 0.361748\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 382000: 0.283186\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 384000: 0.279662\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 386000: 0.284111\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 388000: 0.299582\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 390000: 0.309447\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 392000: 0.269564\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 394000: 0.268984\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 396000: 0.302154\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 398000: 0.313424\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 400000: 0.290583\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 402000: 0.308458\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 404000: 0.315406\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 406000: 0.308385\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 408000: 0.304180\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 410000: 0.286967\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 412000: 0.323537\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 414000: 0.325537\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 416000: 0.264390\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 418000: 0.331186\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 420000: 0.296912\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 422000: 0.301044\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 424000: 0.295388\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 426000: 0.258121\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 428000: 0.286472\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 430000: 0.253229\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 432000: 0.305267\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 434000: 0.280411\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 436000: 0.293162\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 438000: 0.317521\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 440000: 0.295471\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 442000: 0.278158\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 444000: 0.355945\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 446000: 0.246167\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 448000: 0.302742\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 450000: 0.302624\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 452000: 0.279757\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 454000: 0.268833\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 456000: 0.304195\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 458000: 0.259758\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 460000: 0.287047\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 462000: 0.294083\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 464000: 0.305782\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 466000: 0.319270\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 468000: 0.307278\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 470000: 0.318691\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 472000: 0.322122\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 474000: 0.271150\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 476000: 0.285133\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 478000: 0.284945\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 480000: 0.286198\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 482000: 0.305767\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 484000: 0.322300\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 486000: 0.307610\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 488000: 0.289268\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 490000: 0.344289\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 492000: 0.295915\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 494000: 0.300647\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 496000: 0.316372\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 498000: 0.312517\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 500000: 0.297625\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 502000: 0.320504\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 504000: 0.295689\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 506000: 0.274177\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 508000: 0.287339\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 510000: 0.264818\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 512000: 0.323875\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 514000: 0.276172\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 516000: 0.311401\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 518000: 0.322614\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 520000: 0.349204\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 522000: 0.283844\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 524000: 0.308850\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 526000: 0.309087\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 528000: 0.261400\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 530000: 0.301889\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 532000: 0.302055\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 534000: 0.281733\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 536000: 0.286055\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 538000: 0.307667\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 540000: 0.305337\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 542000: 0.325420\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 544000: 0.278712\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 546000: 0.341734\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 548000: 0.291057\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 550000: 0.364905\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 552000: 0.316648\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 554000: 0.294355\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 556000: 0.287511\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 558000: 0.302124\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 560000: 0.262171\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 562000: 0.308756\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 564000: 0.270362\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 566000: 0.288112\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 568000: 0.278898\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 570000: 0.253007\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 572000: 0.319309\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 574000: 0.245153\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 576000: 0.351701\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 578000: 0.320272\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 580000: 0.310993\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 582000: 0.383249\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 584000: 0.337683\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 586000: 0.277296\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 588000: 0.289942\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 590000: 0.286462\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 592000: 0.295094\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 594000: 0.304337\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 596000: 0.269111\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 598000: 0.327884\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 600000: 0.294515\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 602000: 0.261496\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 604000: 0.310729\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 606000: 0.282901\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 608000: 0.281603\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 610000: 0.317830\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 612000: 0.320032\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 614000: 0.310299\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 616000: 0.311959\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 618000: 0.326406\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 620000: 0.322511\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 622000: 0.287270\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 624000: 0.310179\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 626000: 0.313312\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 628000: 0.281111\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 630000: 0.323543\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 632000: 0.281166\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 634000: 0.286845\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 636000: 0.274786\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 638000: 0.304965\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 640000: 0.308864\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 642000: 0.288955\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 644000: 0.309080\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 646000: 0.325885\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 648000: 0.289383\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 650000: 0.328170\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 652000: 0.321119\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 654000: 0.305152\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 656000: 0.334717\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 658000: 0.298684\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 660000: 0.286325\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 662000: 0.250364\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 664000: 0.287081\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 666000: 0.295155\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 668000: 0.289147\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 670000: 0.299840\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 672000: 0.297718\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 674000: 0.328252\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 676000: 0.283408\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 678000: 0.307288\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 680000: 0.307396\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 682000: 0.282911\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 684000: 0.291232\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 686000: 0.372140\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 688000: 0.309788\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 690000: 0.321452\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 692000: 0.320166\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 694000: 0.303850\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 696000: 0.270559\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 698000: 0.317845\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 700000: 0.289066\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 702000: 0.293832\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 704000: 0.296029\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 706000: 0.377502\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 708000: 0.237368\n",
      "Minibatch accuracy: 97.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 710000: 0.340435\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 712000: 0.268287\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 714000: 0.291979\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 716000: 0.294137\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 718000: 0.364657\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 720000: 0.280579\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 722000: 0.324166\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 724000: 0.328419\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 726000: 0.328962\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 728000: 0.312862\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 730000: 0.330514\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 732000: 0.301425\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 734000: 0.308288\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 736000: 0.242018\n",
      "Minibatch accuracy: 97.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 738000: 0.289051\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 740000: 0.289792\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 742000: 0.281025\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 744000: 0.323828\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 746000: 0.327459\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 748000: 0.282674\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 750000: 0.289154\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 752000: 0.276306\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 754000: 0.320934\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 756000: 0.283310\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 758000: 0.313232\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 760000: 0.292895\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 762000: 0.305674\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 764000: 0.302036\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 766000: 0.279021\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 768000: 0.338150\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 770000: 0.288931\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 772000: 0.289385\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 774000: 0.299425\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 776000: 0.308246\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 778000: 0.303480\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 780000: 0.305108\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 782000: 0.282181\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 784000: 0.286210\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 786000: 0.301474\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 788000: 0.320479\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 790000: 0.265594\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 792000: 0.313318\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 794000: 0.326932\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 796000: 0.274024\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 798000: 0.305466\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 800000: 0.270746\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 802000: 0.280818\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 804000: 0.290794\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 806000: 0.263332\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 808000: 0.340397\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 810000: 0.276394\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 812000: 0.285491\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 814000: 0.293221\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 816000: 0.314223\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 818000: 0.285327\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 820000: 0.252166\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 822000: 0.282383\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 824000: 0.298429\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 826000: 0.298424\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 828000: 0.256215\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 830000: 0.288061\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 832000: 0.340371\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 834000: 0.309566\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 836000: 0.281610\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 838000: 0.313017\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 840000: 0.259261\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 842000: 0.295338\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 844000: 0.317174\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 846000: 0.292107\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 848000: 0.282750\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 850000: 0.295429\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 852000: 0.335391\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 854000: 0.277894\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 856000: 0.307258\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 858000: 0.312877\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 860000: 0.292735\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 862000: 0.261147\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 864000: 0.292474\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 866000: 0.333844\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 868000: 0.271060\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 870000: 0.293184\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 872000: 0.276260\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 874000: 0.290969\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 876000: 0.334751\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 878000: 0.318916\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 880000: 0.321036\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 882000: 0.317067\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 884000: 0.339647\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 886000: 0.275235\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 888000: 0.315097\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 890000: 0.304819\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 892000: 0.298475\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 894000: 0.280643\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 896000: 0.280162\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 898000: 0.309033\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 900000: 0.335186\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 902000: 0.343006\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 904000: 0.332679\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 906000: 0.276284\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 908000: 0.251355\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 910000: 0.304311\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 912000: 0.293960\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 914000: 0.254550\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 916000: 0.299737\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 918000: 0.301295\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 920000: 0.324542\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 922000: 0.300612\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 924000: 0.330257\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 926000: 0.333236\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 928000: 0.314515\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 930000: 0.263498\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 932000: 0.268283\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 934000: 0.320014\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 936000: 0.312176\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 938000: 0.322832\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 940000: 0.297690\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 942000: 0.309398\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 944000: 0.375360\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 946000: 0.331364\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 948000: 0.292900\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 950000: 0.304702\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 952000: 0.286573\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 954000: 0.278558\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 956000: 0.311047\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 958000: 0.268109\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 960000: 0.239946\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 962000: 0.270725\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 964000: 0.294804\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 966000: 0.301246\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 968000: 0.321202\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 970000: 0.317254\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 972000: 0.281597\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 974000: 0.330739\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 976000: 0.308239\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 978000: 0.271120\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 980000: 0.309067\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 982000: 0.294776\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 984000: 0.321176\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 986000: 0.303022\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 988000: 0.331376\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 990000: 0.313251\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 992000: 0.282631\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 994000: 0.282768\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 996000: 0.306968\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Current learning rate = 0.00000\n",
      "Minibatch loss at step 998000: 0.281500\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.0%\n",
      "Test accuracy: 96.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000000\n",
    "lr = []\n",
    "mb_acc =[]\n",
    "val_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized with nodes = %d and L2 of %.10f\" % (l1_nodes, betaL2))\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #batch_i = np.random.choice([1,20,300,4000])\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions, l_rate = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            lr.append(l_rate)\n",
    "            mb_acc.append(accuracy(predictions, batch_labels))\n",
    "            val_acc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "        \n",
    "        if (step % 2000 == 0):\n",
    "            print(\"Current learning rate = %.5f\" % l_rate)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            continue\n",
    "\n",
    "    acc = accuracy(test_prediction.eval(), test_labels)  \n",
    "    print(\"Test accuracy: %.1f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc48803250>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAFoCAYAAABuakCAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbZdY0ja5f44AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc48933650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,len(lr)*500,500), val_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
